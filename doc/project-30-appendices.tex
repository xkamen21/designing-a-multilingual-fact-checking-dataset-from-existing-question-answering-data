\newpage
\section*{Appendix A} \label{apendix_a}

\subsection*{Introduction into the task}
The aim of my thesis is to evaluate two different approaches to building a multilingual dataset for fact-checking models.
What does a fact-checking model mean? In simple terms, you can imagine that a fact is passed to the model, for example: ''\textit{President Barack Obama was born in 2012.}`` and the model tries to evaluate whether that fact is \texttt{true} or \texttt{false}. To train such a model, it is important to have a dataset that contains as many examples as possible. Each example must contain a claim, an evidence, that supports or refutes the claim and the correct label (in my case: \texttt{refute}/\texttt{support}). Nowadays, there are already many datasets, but they are all mostly only in English and thus there is a lack of multilingual datasets.

A similar task is a model that answers questions. A question is passed to the model and the model generates an answer. To train such a model, one needs a dataset that contains the question-answer pair, ideally including a record of where the answer to the question occurred.

It can be seen that the approach to training the two models is quite similar. Therefore, I decided to create the fact-checking dataset from another existing multilingual dataset for the question-answering model that contains question-answer pairs. From these pairs, the seq2seq model (a model that has a sequence of words as input and the output is again a sequence of words) was trained to generate a fact, since in the question we have information about what the answer specifically refers to, and the answer itself is the information we want to transfer to the fact.
\begin{table}[h]
    \centering
    \begin{tabular}{cc}
     \textbf{Question}: When was Barack Obama born? & \textbf{Answer}: 1961 \\
    \end{tabular}
    \caption{Example of the Question-Answer pair.}
\end{table}

Since generating a fact is not a trivial task for the model (especially for multilingual data), two different approaches were therefore taken. 
\begin{itemize}
    \item The first approach is to train the model on the English dataset. The translated input (question and answer) is then passed to the model. The model returns the output (fact) in English, which is then translated back into the original language.
    \item  The second approach is to train a multilingual model that receives the input in the desired language and therefore returns the output in the desired language.

\end{itemize}

\subsection*{Task for annotators}
Automatic evaluation of the results of each approach is difficult to perform and therefore human evaluation is also important. In the attached file you have received an excel that contains the generated claim (translated into the english) from both models. These results need to be manually analyzed to determine whether the resulting fact is understandable or not.

Therefore, I would like to ask you to perform a manual analysis. One line of the file corresponds to one example from the dataset. Each line contains information about the original question and its answer. This is followed by the claim that was generated by model A and then the claim that was generated by model B. Your task is to assign to each claim one of your answers at your discretion: whether the claim is correct, whether the claim is wrong, or whether the claim is generated in an interesting way. An interesting claim contains all the necessary information, but is not quite as specific as you would expect.

\begin{table}[h]
    \centering
    \begin{tabular}{c}
        \textbf{Question}: Where was Barack Obama born? \textbf{Answer}: USA\\
        \textbf{Claim}: Barack Obama was born in Honolulu, Hawaii
    \end{tabular}
    \caption{Example of the generated calim from the Question-Answer pair.}
\end{table}

\subsubsection*{ToDo list for annotators}
In summary, you need to add a label to each claim and then select a preference for the claim that you think is better generated:

\begin{enumerate}
    \item fill in the column to the right of the claim with one of the answers:
    \begin{table}[h]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            correct & incorrect & interesting \\
            \hline
        \end{tabular}
    \caption{Labels for the claim validation.}
    \end{table}
    \item fill in the last column with your preference for the generated fact based on the model as follows:
    \begin{table}[h]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            A & B & CBJ \\
            \hline
        \end{tabular}
    \caption{Labels for the claim preference.}
    \end{table}
\end{enumerate}
\noindent * (CBJ stands for: cannot be judged)